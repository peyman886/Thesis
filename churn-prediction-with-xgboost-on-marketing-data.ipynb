{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":782411,"sourceType":"datasetVersion","datasetId":408408}],"dockerImageVersionId":29867,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## THIS NOTEBOOK IS FORKED FROM [THIS KERNEL](https://www.kaggle.com/slayomer/churn-prediction-with-xgboost-on-marketing-data)\n### Here I ONLY made the plots and visualizations more clear and descriptive. The analyses and methodologies are originally done in [Ã–mer Faruk Eker](https://www.kaggle.com/slayomer)'s kernel. So, please upvote me after him!","metadata":{}},{"cell_type":"markdown","source":"# The Complete Journey Dataset Analysis: Churn Prediction\n\nIn this notebook we will make a churn prediction model using a retail dataset. Let's start importing necessary packages for the analysis.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nsns.set_style(\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nThe dataset consist of eight tables in separate .csv files, we will go through six of them which we will use in this notebook. The dataset covers a two year span purchase transactions of 2500 households. Also demographics information of households, campaign and coupon redemption informations are available. In the modeling phase we will join these tables to make our final dataset.\n\n* Campaign Descriptions (campaigndesc.csv)\n* Campaigns (campaign_table.csv)\n* Coupons (coupon.csv)\n* Coupon Redemptions (coupon_redempt.csv)\n* Transactions (transaction_data.csv)\n* Demographics (hh_demographic.csv)\n\n## Campaign Descriptions\nCampaign description data is a look up table, containing the start and the end days of each campaign (30 campaigns in total). Also it gives which campaign belongs to what category (Type A, B and C).","metadata":{}},{"cell_type":"code","source":"dfs = dict()\ndfs[\"campaign_desc\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/campaign_desc.csv\")\ndfs[\"campaign_desc\"].head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Each Campaign Duration\nLet's calculate and visualise how long each campaign lasted within two years period:\n\nCampaign No:15 lasts the longest with a staggering 160 days figure, where other campaigns are fairly close to each other ranging from 30 to 70 days\nAverage campaign duration is 37 days (median)","metadata":{}},{"cell_type":"code","source":"dfs[\"campaign_desc\"][\"DUR\"] = dfs[\"campaign_desc\"].END_DAY-dfs[\"campaign_desc\"].START_DAY\nfig = plt.figure(figsize=(14,6))\nsns.barplot(x=\"CAMPAIGN\",y=\"DUR\",data=dfs[\"campaign_desc\"],orient=\"v\",\n            order=dfs[\"campaign_desc\"].sort_values(by=\"CAMPAIGN\").CAMPAIGN.values)\nplt.title('Duration of each campaign', fontsize=17)\nplt.xlabel('Campaign Number', fontsize=14)\nplt.ylabel('Duration', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Campaign Dataset\nCampaign dataset contains identifying information for the marketing campaigns each household participated in.","metadata":{}},{"cell_type":"code","source":"dfs[\"campaign_table\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/campaign_table.csv\")\ndfs[\"campaign_table\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 2500 households, 1584 of them received a campaign once, the rest never received a campaign.","metadata":{}},{"cell_type":"code","source":"total_households = 2500\nlen(dfs[\"campaign_table\"].household_key.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the other side, a very small portion of the households received double-digit number of campaigns. We will investigate how this will effect churn rates in the following sections. The barchart below groups the households and shows counts of the group populations.","metadata":{}},{"cell_type":"code","source":"no_of_received_campaigns_per_house = dfs[\"campaign_table\"].household_key.value_counts()\nno_of_received_campaigns = no_of_received_campaigns_per_house.value_counts()\nno_of_received_campaigns[0] = total_households-len(no_of_received_campaigns_per_house)\nno_of_received_campaigns = pd.DataFrame(list(zip(no_of_received_campaigns.index,no_of_received_campaigns))\n                                        ,columns=[\"Number of Campaigns Received\",\n                                                  \"Number of Households Reached To\"]).sort_values(\n                                                                                            by=\"Number of Campaigns Received\",ascending=True)\nplt.figure(figsize=(12,6))\nsns.barplot(y=\"Number of Campaigns Received\",x=\"Number of Households Reached To\",data=no_of_received_campaigns,orient=\"h\",order=no_of_received_campaigns[\"Number of Campaigns Received\"])\nplt.title('Number of Campaigns Received By Households Number', fontsize=17)\nplt.xlabel('Number of Households Reached To', fontsize=14)\nplt.ylabel('Number of Campaigns Received', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most Frequent Campaigns\nGraph below gives the most frequent campaigns. Campaign number 18, 13 and 8 being the most frequent ones reaching 1000 and more households each (no campaigns applied to same customer more than once therefore Frequency axis gives unique number of households).","metadata":{}},{"cell_type":"code","source":"freq_campaigns = pd.DataFrame(list(zip(dfs[\"campaign_table\"].CAMPAIGN.value_counts().index, \n                                       dfs[\"campaign_table\"].CAMPAIGN.value_counts())),columns=[\"Campaign\",\"Frequency\"])\nfig = plt.figure(figsize=(12,8))\nsns.barplot(y=\"Campaign\",x=\"Frequency\",data = freq_campaigns,orient=\"h\")\nplt.title('Frequency of Each Campaign', fontsize=17)\nplt.xlabel('Frequency', fontsize=14)\nplt.ylabel('Campaign No.', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Coupons\nIs a lookup table which lists all the coupons sent to customers as part of a campaign, as well as the products for which each coupon is redeemable","metadata":{}},{"cell_type":"code","source":"dfs[\"coupon\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/coupon.csv\")\ndfs[\"coupon\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Coupon Redemptions\nIs a data table, ordered by household_key, gives which household redeemed what coupon number. Also the day of the redeem and the campaign number is given for each household.","metadata":{}},{"cell_type":"code","source":"dfs[\"coupon_redempt\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/coupon_redempt.csv\")\ndfs[\"coupon_redempt\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 2500 households, only 434 of them (17%) redeemed coupons within this period.","metadata":{}},{"cell_type":"code","source":"len(dfs[\"coupon_redempt\"].household_key.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of Redeems for each Campaign\nMost frequent campaigns (18, 13 and 8, all belongs to TypeA) attracts the most number of redeems as expected. The barchart below gives the number of redeems per campaign.","metadata":{}},{"cell_type":"code","source":"redeem_frequency = pd.DataFrame(list(zip(dfs[\"coupon_redempt\"].CAMPAIGN.value_counts().index, dfs[\"coupon_redempt\"].CAMPAIGN.value_counts())),columns=[\"Campaign\",\"No of Redeems\"])\nfig = plt.figure(figsize=(14,8))\nsns.barplot(y=\"Campaign\",x=\"No of Redeems\",data = redeem_frequency,orient=\"h\",order=redeem_frequency.Campaign)\nplt.title('Number of Redeems Each Campaign', fontsize=17)\nplt.xlabel('No of Redeems', fontsize=14)\nplt.ylabel('Campaign No.', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The barchart below gives the redeem rates of each campaign in a descending order. Campaign 13 and 18 are clearly have higher redeem rates (~60%) as compared to the rest of campaigns.\n\nBased on these results, campaign organisers did well by promoting the right campaigns (13 and 18) more frequently.","metadata":{}},{"cell_type":"code","source":"\nredems_per_camp = dfs[\"coupon_redempt\"].CAMPAIGN.value_counts().sort_values()/dfs[\"campaign_table\"].CAMPAIGN.value_counts().sort_values()\nredems_per_camp = pd.DataFrame(list(zip(redems_per_camp.index,redems_per_camp)),\n                               columns=[\"Campaign\",\"Redeem_Rate\"]).sort_values(by=\"Redeem_Rate\",ascending=False)\nplt.figure(figsize=(12,8))\nsns.barplot(y=\"Campaign\",x=\"Redeem_Rate\",data=redems_per_camp,orient=\"h\",order=redems_per_camp.Campaign)\nplt.title('Redeem Rate per Campaign', fontsize=17)\nplt.xlabel('Redeem Rate', fontsize=14)\nplt.ylabel('Campaign No.', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transactional Data\nTransactional data contains purchase history of each household. It contains the product ID and sales value, store ID and all other transactional features.","metadata":{}},{"cell_type":"code","source":"dfs[\"transaction_data\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/transaction_data.csv\")\ndfs[\"transaction_data\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now lets do some descriptive analysis on the transactional data. First, let's group the data by household numbers:","metadata":{}},{"cell_type":"code","source":"grouped_sum = dfs[\"transaction_data\"].groupby(\"household_key\").sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average amount of purchase by a household within these two years is:","metadata":{}},{"cell_type":"code","source":"av_purc = round(dfs['transaction_data'].SALES_VALUE.sum()/len(grouped_sum),1)\nav_purc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average total number of products purchased by a household within two years is:","metadata":{}},{"cell_type":"code","source":"av_tot_prod = round(grouped_sum[\"QUANTITY\"].mean(),1)\nav_tot_prod","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average total number of unique products purchased by a household within two years is:","metadata":{}},{"cell_type":"code","source":"av_uniq_prod = round(len(dfs[\"transaction_data\"].groupby([\"household_key\",\"PRODUCT_ID\"]).sum()[\"QUANTITY\"])/len(grouped_sum),1)\nav_uniq_prod","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average number of store visits per household within two years is:","metadata":{}},{"cell_type":"code","source":"av_days_visited = round(len(dfs[\"transaction_data\"].groupby([\"household_key\",\"DAY\"]).count())/len(grouped_sum),1)\nav_days_visited","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Stores\nTop 20 stores based on total sales amount (USD) is calculated and plotted below. Stores with IDs of 367 and 406 made the most sales amongst 582 stores with over $200,000 each.","metadata":{}},{"cell_type":"code","source":"sales_per_store = dfs[\"transaction_data\"].groupby(\"STORE_ID\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\nsales_per_store = pd.DataFrame(list(zip(sales_per_store.index,sales_per_store)),columns=[\"Store ID\",\"Total Sales (USD)\"])\nfig_store = plt.figure(figsize=(14,8))\nsns.barplot(y=\"Store ID\",x=\"Total Sales (USD)\",data = sales_per_store[:20],order=sales_per_store[:20][\"Store ID\"],orient=\"h\")\nplt.title('Top 20 Stores based on Sale Amount', fontsize=17)\nplt.xlabel('Total Sales (USD)', fontsize=14)\nplt.ylabel('Store ID', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Top 20 customers based on total purchase amount (USD)\nTop customer is with the ID of 1023 who made the most purchases amongst 2500 house holds with almost $40,000.","metadata":{}},{"cell_type":"code","source":"purc_per_cust = dfs[\"transaction_data\"].groupby(\"household_key\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\npurc_per_cust = pd.DataFrame(list(zip(purc_per_cust.index,purc_per_cust)),columns=[\"household_key\",\"Total Purchase (USD)\"])\nfig_store = plt.figure(figsize=(14,8))\nsns.barplot(y=\"household_key\",x=\"Total Purchase (USD)\",data = purc_per_cust[:20],order=purc_per_cust[:20][\"household_key\"],orient=\"h\")\nplt.title('Top 20 Customers based on Purchase Number', fontsize=17)\nplt.xlabel('Total Purchases (USD)', fontsize=14)\nplt.ylabel('Household Key', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Demographics Data\nDemographics data contains household demographical information such as age group, marital status and househols size. We will be using this table as a base and add few columns after feature engineering. Target variable for the churn prediction will be added in the final dataset before moving on to the predictive modelling.","metadata":{}},{"cell_type":"code","source":"dfs[\"hh_demographic\"] = pd.read_csv(\"/kaggle/input/dunnhumby-the-complete-journey/hh_demographic.csv\")\ndfs[\"hh_demographic\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets first start defining the functions for exploratory analysis of demographic variables:","metadata":{}},{"cell_type":"code","source":"def pie_categorical(data):\n    #function to plot the histogram of categorical variables in pie graph\n    features = data.columns\n    #plot pie charts of categorical variables\n    fig_pie_cat = plt.figure(figsize=(15,15))\n    count = 1\n    #calculate dynamic numbers of subplot rows and columns\n    cols = int(np.ceil(np.sqrt(len(features))))\n    rows = int(np.ceil(len(features)/cols))\n    for i in features:\n        ax = fig_pie_cat.add_subplot(rows,cols,count)\n        data[i].value_counts().plot(kind=\"pie\",autopct=\"%.1f%%\",ax=ax)\n        plt.ylabel(\"\")\n        plt.title(i,fontweight=\"bold\",fontsize=8)\n        count += 1\n\ndef hist_numeric(data):\n    #function to plot the histogram of numeric variables\n    features = data.columns\n    fig_hists = plt.figure(figsize=(15,15))\n    fig_hists.subplots_adjust(hspace=0.5,wspace=0.5)\n    count = 1\n    #calculate dynamic numbers of subplot rows and columns\n    cols = int(np.ceil(np.sqrt(len(features))))\n    rows = int(np.ceil(len(features)/cols))\n    for i in features:\n        ax = fig_hists.add_subplot(rows,cols,count)\n        data[i].plot(kind=\"hist\",alpha=.5,bins=25,edgecolor=\"navy\",legend=False,ax=ax)\n        ax.set_xlabel(\"\")\n        ax.set_title(i,fontweight=\"bold\",fontsize=10)\n        count += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nSince all variables are categorical in the demographics dataset, we will use our _piecategorical function. The pie charts below gives the distribution of each categorical variable in the demographics dataset.\n\nSome interesting findings:\n\n* Majority of the customers age in between 35-54 (~60%)\n* Married couples are almost 3 times the singles\n* Almost half of the population have a yearly salary between $35-74K\n* Majority of the customers own a house (63%)\n* Majority of the customers does not have a child (~70%)","metadata":{}},{"cell_type":"code","source":"pie_categorical(dfs[\"hh_demographic\"].drop(\"household_key\",axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Churn Prediction\nLet's start working on the \"Churn Prediction\" problem from here.\n\nDatasets have been examined and found out that it does not contain a column which indicates whether customer(household) is churned or not. Therefore we will have to define our own churn definition and move on to the modelling.\n\nChurn rate works well for subscription-based products or ones that have regular nature of interactions like Netflix subscription. Itâ€™s clear that customer has churned at the moment when he cancels or misses the next planned payment.\n\nWhen we know which customers have churned we can ask them for reasons and prioritize fixes for them. But for not regular, transactional products churn rate is hard to measure since we donâ€™t know which customers are churned and which are dormant.\n\nA generally accepted retail churn rate is between five to sevent percent per year. Less than five percent is a great goal, but a churn rate over ten percent is cause for concern. Even as you acquire more customers, your business can't grow unless you have a greater volume of incoming customers than outgoing ones.\n\nSatisfying existing customers is actually more profitable than obtaining new ones. **It costs five times more to obtain a new customer than it does to retain an existing customer**. Decreasing your churn rate by five percent increases profits up to 125%.\n\nLet's define transactional churn:\n\n**A customer will be considered as churned if not purchased from a store 2 weeks or more.**\n\n2 weeks out threshold is chosen as it splits households around 85%/15% No Churn/Churn.","metadata":{}},{"cell_type":"code","source":"out_weeks_threshold = 2\n\n#weekly customer purchase amount\nweekly_purchase = dfs[\"transaction_data\"].groupby([\"household_key\",\"WEEK_NO\"]).sum()[\"SALES_VALUE\"]\nweekly_purchase = weekly_purchase.unstack()\n\n#customer churned by their last 2 week's purchasing behaviour\n#go through all households and calculate out weeks\ntarget = []\ncommon_houses = set(dfs[\"hh_demographic\"].household_key) & set(dfs[\"transaction_data\"].household_key)\nfor house in common_houses:\n  target.append(102-weekly_purchase.loc[house].dropna().index[-1])\n\ntarget = pd.DataFrame(list(zip(common_houses,target)),columns=[\"household_key\",\"No. of Churned Weeks\"])\nplt.figure(figsize=(10,6))\nax = sns.distplot(target[\"No. of Churned Weeks\"],kde=False)\nax.axvline(x=out_weeks_threshold,c=\"red\",label=\"Out weeks threshold\")\nax.legend()\nax.set_yscale(\"log\")\nax.set_ylabel(\"Frequency (Log)\", fontsize=14)\nax.set_xlabel(\"No. of Churned Weeks\", fontsize=14)\n\nplt.title('Churn Weeks Number', fontsize=17)\n# plt.xlabel('Total Purchases (USD)', fontsize=14)\n# plt.ylabel('Household Key', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets generate our target variable for modelling:\n\n* Churned -> True\n* Not Churned -> False","metadata":{}},{"cell_type":"code","source":"target[\"isChurned\"] = target[\"No. of Churned Weeks\"]>=out_weeks_threshold\ntarget.drop(\"No. of Churned Weeks\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how households split with the defined churn:","metadata":{}},{"cell_type":"code","source":"target.isChurned.value_counts().plot(kind=\"pie\",autopct=\"%.1f%%\",labels=[\"Non-Churned\",\"Churned\"])\nplt.ylabel(\"\")\nplt.title(\"Churn Variable Distribution\",fontweight=\"bold\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature engineering\nLet's generate some features from the \"Campaign Table\", \"Transaction Data\" and \"Coupon Redempt\" tables to be used in the churn prediction model:\n\n**Feature 1**: List of campaigns received by each household:","metadata":{}},{"cell_type":"code","source":"household_per_campaign = dfs[\"campaign_table\"].groupby(\"CAMPAIGN\")[\"household_key\"].apply(list)\ntotal_campaigns = len(dfs[\"campaign_table\"].CAMPAIGN.unique())\ndf_camp = pd.DataFrame(np.full((total_households,total_campaigns),0),columns=[\"Camp_\"+str(i) for i in range(1,total_campaigns+1)],\n                                                                                                           index=range(1,total_households+1))\nfor camp in household_per_campaign.index:\n    df_camp.loc[household_per_campaign[camp],\"Camp_\"+str(camp)] = 1\n\ndf_camp[\"household_key\"] = df_camp.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature 2**: Total number of received campaigns per household:","metadata":{}},{"cell_type":"code","source":"temp = dfs[\"campaign_table\"].household_key.value_counts().sort_index()\nno_of_received_campaigns = pd.DataFrame(list(zip(temp.index,temp.values.astype(int))),columns=[\"household_key\",\"no_of_received_campaigns\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLets see if the \"number\" of campaigns being received affects a household's purchase behaviour.\n\nThe correlation between the number of campaigns received by a household and their total purchase amount is visualised in a scatter plot below. Linear line in the graph shows the regression model fitted to the scattered data. One can say the aforementioned variables are linearly related.","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(14,4))\nno_of_received_campaigns_vs_sumsales = pd.merge(no_of_received_campaigns,purc_per_cust,on=\"household_key\")\nsns.lmplot(x=\"no_of_received_campaigns\",y=\"Total Purchase (USD)\",data=no_of_received_campaigns_vs_sumsales,\n                                                                                                          size=6, aspect=1.3)\nplt.title('Correlation between Total Purchases and Number of Recieved Campaigns', fontsize=17)\nplt.xlabel('Total Purchases (USD)', fontsize=14)\nplt.ylabel('Number of Recieved Campaigns', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_received_campaigns_vs_sumsales.drop(\"household_key\",axis=1).corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatter plot and the correlation value above (71%) indicates that the total purchase amount of a household is positively correlated to the number of campaigns received by them.\n\n**Feature 3**: List of campaigns resulted in coupon redemption:","metadata":{}},{"cell_type":"code","source":"temp = dfs[\"coupon_redempt\"].groupby(\"household_key\")[\"CAMPAIGN\"].apply(list)\nrede_camp_history_per_household = pd.DataFrame(list(zip(temp.index,temp.values)),columns=[\"household_key\",\"redeemed_CAMPAIGN_list\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature 4**: The number of redemptions made by each household:","metadata":{}},{"cell_type":"code","source":"temp = dfs[\"coupon_redempt\"].household_key.value_counts().sort_index()\nno_of_rede_per_household = pd.DataFrame(list(zip(temp.index,temp.values.astype(int))),columns=[\"household_key\",\"no_of_redeems\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature 5**: Most Frequent Campaign Type (A,B,C) received by each household:","metadata":{}},{"cell_type":"code","source":"camp_list = dfs[\"campaign_table\"].groupby(\"household_key\")[\"DESCRIPTION\"].apply(list)\nmost_freq_type = pd.DataFrame(list(zip(camp_list.index,[pd.Series(i).value_counts().idxmax() for i in camp_list])),columns=[\"household_key\",\"most_freq_camp_type\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature 6**: Top 20 stores with high number of households which have more high out weeks:","metadata":{}},{"cell_type":"code","source":"store_per_house = dfs[\"transaction_data\"].groupby(\"household_key\")[\"STORE_ID\"].apply(list).apply(np.unique)\n\nchurn_houses = set(target.household_key[target.isChurned == True].sort_values().values) & set(dfs[\"hh_demographic\"].household_key.unique())\nchurn_stores = []\nfor i in churn_houses:\n  churn_stores.extend(store_per_house[i])\n\nchurn_stores = pd.Series(churn_stores).value_counts()/len(churn_houses)\n\ndf_store = dfs[\"transaction_data\"].groupby([\"STORE_ID\",\"household_key\"]).count()[\"BASKET_ID\"].unstack(\"STORE_ID\")\ndf_store.replace(np.nan,0,inplace=True)\ndf_store[df_store>0] = 1\n\n#select the top stores with high number of churners\ndf_store = df_store.loc[:,churn_stores.index[:20]]\ndf_store.columns=[\"Store_\"+str(i) for i in df_store.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature 7: Amount of purchase of a household within two years:","metadata":{}},{"cell_type":"code","source":"purc_per_cust = dfs[\"transaction_data\"].groupby(\"household_key\").sum()[\"SALES_VALUE\"].sort_values(ascending=False)\npurc_per_cust = pd.DataFrame(list(zip(purc_per_cust.index,purc_per_cust)),columns=[\"household_key\",\"Total Purchase (USD)\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Last touches on the dataset\nGenerate the final dataframe to be used for predictive modelling","metadata":{}},{"cell_type":"code","source":"data = pd.merge(dfs[\"hh_demographic\"],df_camp,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,no_of_received_campaigns,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,df_store,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,no_of_rede_per_household,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,most_freq_type,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,purc_per_cust,on=\"household_key\",how=\"left\")\ndata = pd.merge(data,target,on=\"household_key\",how=\"left\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have 91 features and a target variable in the final dataframe","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change NaN or None/Unknown values to zero for the columns of \"number of received campaigns\", \"kid category\" and the \"number of redeems\"","metadata":{}},{"cell_type":"code","source":"data.no_of_received_campaigns.replace(np.nan,0,inplace=True)\ndata.no_of_received_campaigns = data.no_of_received_campaigns.astype(int)\ndata.no_of_redeems.replace(np.nan,0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change other object type elements in the columns to integer for modeling purpose","metadata":{}},{"cell_type":"code","source":"data.KID_CATEGORY_DESC.replace([\"None/Unknown\",\"3+\"],[0,3],inplace=True)\ndata.KID_CATEGORY_DESC = data.KID_CATEGORY_DESC.astype(int)\ndata.HOUSEHOLD_SIZE_DESC.replace(\"5+\",5,inplace=True)\ndata.HOUSEHOLD_SIZE_DESC = data.HOUSEHOLD_SIZE_DESC.astype(int)\ndata[\"Total Purchase (USD)\"] = data[\"Total Purchase (USD)\"].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Household_key is the ID of a household, therefore will not be used in the modelling. The rest of the data will be used.","metadata":{}},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how our features interact with the target variable. Start with the \"Age\" variable:\n\n* Households with the age of 55-64 tend to churn less then the rest of other age groups\n* No monotonic increase or decrease observed with age","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(x=\"AGE_DESC\",y=\"isChurned\",data=data,order=[\"19-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\",\"65+\"])\nax.axhline(y=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Marital Status vs Churn rate:\n\n* Married couples (Group A) tend to churn more compared to singles (Group B)\n* Unknown (Group U) households increase the chun rate of the population","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(x=\"MARITAL_STATUS_CODE\",y=\"isChurned\",data=data,order=[\"A\",\"B\",\"U\"])\nax.axhline(y=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Income vs Churn Rate:\n\n* No monotonic increase or decrease observed over the income groups\n* However as seen in the barchart below, no churn was observed when a household's income reaches to 175K and more (27 samples out of 801 households)","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,5))\nax = sns.barplot(x=\"INCOME_DESC\",y=\"isChurned\",data=data,\n                 order=[\"Under 15K\",\"15-24K\",\"25-34K\",\"35-49K\",\"50-74K\",\"75-99K\",\"100-124K\",\"125-149K\",\"150-174K\",\"175-199K\",\"200-249K\",\"250K+\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.axhline(y=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Home Ownership vs Churn Rate:\n\n* No significant differences between owners and renters","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=\"HOMEOWNER_DESC\",x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of Redeems vs Churn Rate:\n\nLets group our data based on household's number of redeems. We will use pandas.cut which groups the number of redeems into the bins. Graph below separates 0 redeemers, redeemed once and the final group redeemed more than once:\n\n* Churn rate decreases as the number of redeems increase","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=pd.cut(data.no_of_redeems,bins=[-0.1,0,1,35],duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Churn rate vs Number of campaigns received by a household\n\n* Churn rate decreases as the number of campaigns received by a household increases","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=pd.qcut(data.no_of_received_campaigns,5,duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not very clear, however churn rate tends to decrease when children starts to appear in households","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=\"HH_COMP_DESC\",x=\"isChurned\",data=data,orient=\"h\",order=['Unknown','Single Female','Single Male','2 Adults No Kids','2 Adults Kids','1 Adult Kids'])\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Churn rate tends to decrease as the number of family members increases","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=\"HOUSEHOLD_SIZE_DESC\",x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Campaign Type vs Churn rate:\n\n* TypeC campaign fails as all households which was approached with campaign C are churned. However note that it has extremely lower sample size (1% of all population)\n* TypeB is the most successful one with the lowest churn rate","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=\"most_freq_camp_type\",x=\"isChurned\",data=data,orient=\"h\",order=['TypeA','TypeB','TypeC'])\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total purchase of a household vs Churn rate:\n\n* Total purchase amount of a customer is a clear separator, expected to perform well as a feature in the churn prediction model\n* Churn rate decreases as total amount of purchase of a household (within 2 years) increases","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=pd.qcut(data[\"Total Purchase (USD)\"],5,duplicates=\"drop\"),x=\"isChurned\",data=data,orient=\"h\")\nax.axvline(x=data.isChurned.sum()/len(data),c=\"red\",label=\"population mean\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation between Campaign Number and the Target (isChurned) are calculated and displayed in the barchart below:\n\n* Campaign 25 as a feature expected to be performing well in the model as it has the highest correlation with the target variable","metadata":{}},{"cell_type":"code","source":"corrs = abs(data[[\"Camp_\"+str(i) if i<31 else \"isChurned\" for i in range(1,32)]].corr()[\"isChurned\"])\ncorrs.drop(\"isChurned\",inplace=True)\ncorrs = pd.DataFrame(list(zip(corrs.index,corrs)),columns=[\"Campaign No.\",\"Correlation with Target\"]).sort_values(by=\"Correlation with Target\",ascending=False)\nfig_store = plt.figure(figsize=(10,8))\nsns.barplot(y=\"Campaign No.\",x=\"Correlation with Target\",data = corrs,orient=\"h\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation between the stores with high number of churner customers and the Target (isChurned) are calculated and displayed in the barchart below:","metadata":{}},{"cell_type":"code","source":"cols =list(df_store.columns)\ncols.extend([\"isChurned\"])\ncorrs = abs(data[cols].corr()[\"isChurned\"])\ncorrs.drop(\"isChurned\",inplace=True)\ncorrs = pd.DataFrame(list(zip(corrs.index,corrs)),columns=[\"Store ID\",\"Correlation with Target\"]).sort_values(by=\"Correlation with Target\",ascending=False)\nfig_store = plt.figure(figsize=(10,6))\nsns.barplot(y=\"Store ID\",x=\"Correlation with Target\",data = corrs,orient=\"h\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ML Model Training & Testing\nIn this section we will train a Machine Learning model with the training data. The machine learning model is chosen as XGBoost (Extreme Gradient Boosting) as they are known to be performing well with imbalanced datasets like ours.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\n#one hot encoding categorical data for modelling\nencoded = pd.get_dummies(data[data.columns[data.dtypes==object]])\ndata_encoded = pd.concat([encoded, data[data.columns[data.dtypes != object]]],axis=1)\n\n#Features (X) and the target (y)\nX = data_encoded.drop(\"isChurned\",axis=1)\ny = data_encoded.isChurned\n\n#lets start with the default hyperparameters and hold-out mechanism for train/test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n#XGBoost\nxgb_mdl = XGBClassifier().fit(X_train.values,y_train.values)\nxgb_mdl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first transformed our categorical variables using one-hot encoding algorithm (get_dummies) to be able to use in our classifier. Then separated the data into train and test 75%/25%. Then generated an XGBoost classifier with it's default parameters and trained it with the training set.\n\nNow let's test our trained classifier with test data:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, average_precision_score\n\n#XGBoost\ny_pred_train = xgb_mdl.predict(X_train.values)\ny_pred = xgb_mdl.predict(X_test.values)\n\nprint(\"Train Data Classification Report:\\n\")\nprint(classification_report(y_train,y_pred_train))\n\nprint(\"Test Data Classification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\n#generate a confusion matrix to visualise precision, recall, misclassification and false alarms\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = list(set(y)), columns = list(set(y)))\n\n#visualise the confusion matrix in the heatmap form\nplt.figure()\nsns.heatmap(cm, annot = True, fmt=\"d\",\n            cmap=sns.color_palette(\"GnBu\")).set(xlabel='predicted values', \n                                                ylabel='real values', \n                                                title = 'Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_test,y_pred)\n# average_precision_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though accuracy for the test set is 88% it is misleading as our target variable is skewed towards not churned (86% are not churned). Even a very basic model which selects majority class all times would score 86% accuracy.\n\nTherefore we will have to focus on how well our model performs on the minority class (churned households). On the test set we have 201 samples only 23 of them being churned. Our model could not manage to detect any of them, therefore test set recall has come out as 0%. This is the part we need to aim to increase.\n\nNow lets try to enhance our model by optimising hyperparameters by using \"Randomised Search\" with \"Cross-Validation\".\n\nWe need to determine the evaluation criteria (scorer) to be optimised for the Randomised Search. Lets see the available scorers:\n\n* average precision, balanced accuracy, roc_auc, f1 are the scorers which are commonly used for imbalanced dataset classifications as they handle situation well as compared to other metrics","metadata":{}},{"cell_type":"code","source":"import sklearn\nsorted(sklearn.metrics.SCORERS.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n\n#handle class imbalance problem by undersampling (not used)\n# no_churn_down_sampled = data_encoded[data_encoded.isChurned == False].sample(sum(data_encoded.isChurned))\n# data_encoded_balanced = pd.concat([no_churn_down_sampled,data_encoded[data_encoded.isChurned]])\n#shuffle the dataset to avoiding sampling biases\n# data = data_encoded_balanced.sample(frac=1)\n\n#shuffle the dataset to avoiding sampling biases\ndata = data_encoded.sample(frac=1)\nX = data.drop(\"isChurned\",axis=1)\ny = data.isChurned\n\n#generate an XGB classifier\nmdl = XGBClassifier()\n\n#parameter ranges\nparam_list = {\n    'silent': [False],\n    'max_depth': range(2,51),\n    'learning_rate': [0.001, 0.01, 0.1, 0.15],\n    'subsample': np.arange(0,1.1,.1),\n    'colsample_bytree': np.arange(0,1.1,.1),\n    'colsample_bylevel': np.arange(0,1.1,.1),\n    'min_child_weight': [0.5, 0.7, 1.0, 2.0, 3.0],\n    'gamma': [0, 0.25, 0.5, 0.75, 0.9, 1.0],\n    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0],\n    'n_estimators': [2, 5, 10, 20, 50, 100],\n    'scale_pos_weight': [1, 1.5, 2, 6, 6.1, 6.3, 6.5, 8],\n    'max_delta_step': [1, 2, 3, 5, 10]\n}\n\nkfold = 5\ncv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=10)\n\n#Randomized Search\n# cv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"roc_auc\",n_jobs=-1).fit(X.values,y.values)\ncv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"balanced_accuracy\",n_jobs=-1).fit(X.values,y.values)\n# cv = RandomizedSearchCV(mdl,param_list,cv=cv_strat,n_iter=100,verbose=1,scoring=\"average_precision\",n_jobs=-1).fit(X.values,y.values)\n\n#use the best estimator after the hyperparameter optimisation\nmdl_best = cv.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter optimisation is completed. Now lets train and test the XGBoost Classifier with the optimised hyperparameters","metadata":{}},{"cell_type":"code","source":"cv.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_scale_pos_weight_ is the parameter used for biasing (weighing more) of the minority class samples. The higher the value the more bias towards minority class. In our dataset, the ratio between majority class samples and the minority class is 6.15 which coheres with this parameter (optimised value came out as 6).\n\nLets test our optimised model with the data:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n\ncv_strat = RepeatedStratifiedKFold(n_splits=kfold,n_repeats=20)\nscores = cross_validate(mdl_best,X.values,y.values,cv=cv_strat,verbose=3,n_jobs=-1,return_train_score=True,\n                        scoring={\"roc_auc\":\"roc_auc\",\n                                 \"recall\":\"recall\",\n                                 \"precision\":\"precision\",\n                                 \"accuracy\":\"accuracy\",\n                                 \"balanced_accuracy\":\"balanced_accuracy\",\n                                 \"average_precision\":\"average_precision\"}) \n\npd.DataFrame(pd.DataFrame(scores).mean(),columns=[\"Score\"]).drop([\"fit_time\",\"score_time\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though getting lower accuracy in total, now we are getting much better results for the minority class as recall and ROC AUC (receiver operating characteristics, area under curve) increased significantly.\n\nThe score table above gives all metric results for both test and train sets. Train and test results are close which is an indication that our model did not overfit.\n\nLet's check which parameters were more important to separate churners/non churners. The barchart below list the features ordered by their importance values for the XGB Classifier.\n\nAs expected, \"Total Purchase\", \"Number of Received Campaigns\", \"Campaign no. 25\", and \"Numer of redeems\" features are the most important features for the classifier. As we recall from feature engineering section, these features were either a good separator or highly correlated to the target variable.","metadata":{}},{"cell_type":"code","source":"feat_imp = pd.DataFrame(list(zip(data.columns,mdl_best.feature_importances_)),columns=[\"Feature\",\"Importance\"]).sort_values(by=\"Importance\",ascending=False)\nfig_store = plt.figure(figsize=(10,20))\nsns.barplot(y=\"Feature\",x=\"Importance\",data = feat_imp,orient=\"h\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}